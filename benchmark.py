#!/usr/bin/env python3

import argparse
import os
import shutil
import subprocess
from pathlib import Path

import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Alignment


def abspath(path: str) -> Path:
    return Path(path).expanduser().resolve()


# Runs a command with specific environment variables.
def run(cmd: str, env: dict = None, action: str = "RUN", debug: bool = False):
    print(f"[{action}] {' '.join(map(str, cmd))}")

    if debug:
        subprocess.run(cmd, env=env, check=True)
    else:
        subprocess.run(cmd, env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)


# Compilation configurations for benchmarking RopSched. The first element of the tuple is the scheduler to use (-misched=...)
# while the second controls whether post-RA scheduling is enabled.
CONFIGS = {
    "Original": ("default", "false"),
    "Pre-RA": ("ropsched", "false"),
    "Post-RA": ("default", "true"),
    "Both": ("ropsched", "true"),
}

# These are the repositories that are benchmarked. Keys are directory names while the values are
# the relative paths of where the final binary is located after building the repo.
BENCHMARK_TARGETS =  {
    "mimalloc": "libmimalloc.so.3.0",
    "chocolate-doom": "src/chocolate-doom",
    "zlib": "libz.so.1.3.1"
}

CLANG = abspath("llvm-ropsched/build/bin/clang")
GSA = abspath("GadgetSetAnalyzer")
BENCHMARKS = abspath("samples")
BINARIES_DESTINATION = BENCHMARKS.parent / "binaries"
RESULTS = abspath("results")


# Generates the CMake files for a repository and builds it for each configuration.
def build_benchmark(repository: Path, flags: str, debug: bool = False) -> list[Path]:
    resulting_binaries = []

    for config, misched_flags in CONFIGS.items():
        build_directory = repository / f"build/{config}"
        build_directory.mkdir(parents=True, exist_ok=True)
        config_flags = flags + f"-mllvm -enable-misched=true -mllvm -misched={misched_flags[0]} -mllvm -misched-postra={misched_flags[1]}"

        # Set up the flags.
        env = os.environ.copy()
        env["CC"] = str(CLANG)
        env["CXX"] = str(CLANG)
        env["CFLAGS"] = config_flags
        env["CXXFLAGS"] = config_flags

        # Generate CMake file and build the benchmark.
        run(["cmake", "-S", str(repository), "-B", str(build_directory)], env=env, action=" CMAKE ", debug=debug)
        run(["cmake", "--build", str(build_directory)], env=env, action=" BUILD ", debug=debug)

        # Copy the resulting binary to a separate folder for uniformity.
        binary_path = build_directory / BENCHMARK_TARGETS[repository.name]
        destination_path = BINARIES_DESTINATION / f"{repository.name}.{config}"
        run(["cp", str(binary_path), str(destination_path)], action=" COPY  ")

        resulting_binaries.append(destination_path)

    return resulting_binaries



# Runs GadgetSetAnalyzer for each configuration.
def compare_benchmark(name: str, binaries: list[Path], debug: bool = False) -> Path:
    original = str(binaries[0])
    variants = [f"{variant.suffix[1:]}={variant}" for variant in binaries[1:]]
    old_results = str(RESULTS / name)

    if os.path.isdir(old_results):
        shutil.rmtree(old_results)

    run(["python3", GSA / "src/GSA.py", original, "--variants", *variants, "--output_metrics", "--result_folder_name", name], action="COMPARE", debug=debug)

    return abspath("results") / f"{name}/Gadget Quality.csv"


# Concatentates all the separate CSV files generated by GadgetSetAnalyzer into a single Excel workbook (.xlsx).
def concatenate_results(results: list[Path]):
    dataframes = []

    for csv in results:
        df = pd.read_csv(csv)
        df.insert(0, "Benchmark", csv.parent.name)
        dataframes.append(df)

    data = pd.concat(dataframes, ignore_index=True)

    output = abspath("results.xlsx")
    data.to_excel(output, index=False)
    wb = load_workbook(output)
    ws = wb.active

    start_row = 2
    current_benchmark = ws.cell(row=2, column=1).value

    for row in range(2, ws.max_row + 2):
        benchmark = ws.cell(row=row, column=1).value

        if benchmark != current_benchmark:
            ws.merge_cells(start_row=start_row, start_column=1, end_row=row - 1, end_column=1)
            ws.cell(row=start_row, column=1).alignment = Alignment(vertical="center", horizontal="center")

            current_benchmark = benchmark
            start_row = row

    for row in ws.iter_cols(3):
        row_name = row[0].value

        for cell in filter(lambda cell: cell.value is not None, row[1:]):
            value = str(cell.value)

            if ("+" in value and "Gadget Quality" in row_name) or ("-" in value and "Number" in row_name):
                cell.style = "Good"
            elif ("-" in value and "Gadget Quality" in row_name) or ("+" in value and "Number" in row_name):
                cell.style = "Bad"
            elif "(" in value:
                cell.style = "Neutral"

    wb.save(output)


def main():
    parser = argparse.ArgumentParser(description="Benchmark LLVM scheduler configurations with GadgetSetAnalyzer.")
    parser.add_argument("-b", "--benchmarks", default="all", help="Benchmarks to run (comma-separated list)")
    parser.add_argument("--flags", default="-O2", help="Clang flags to use when compiling benchmarks")
    parser.add_argument("-d", "--debug", action="store_true", help="Show command output")
    args = parser.parse_args()

    benchmarks = [repository for repository in BENCHMARKS.iterdir()]

    if args.benchmarks != "all":
        benchmarks = filter(lambda benchmark: benchmark.name in args.benchmarks.split(","), benchmarks)

    results = []

    for benchmark in benchmarks:
        print(f"\nBenchmark: {benchmark.name}")
        binaries = build_benchmark(benchmark, args.flags, args.debug)
        output = compare_benchmark(benchmark.name, binaries, args.debug)
        results.append(output)

    concatenate_results(results)


if __name__ == "__main__":
    main()

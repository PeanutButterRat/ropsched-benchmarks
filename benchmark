#!/usr/bin/env python3
import argparse
import os
import platform
import shutil
import subprocess
import sys
from collections.abc import Callable
from copy import copy
from datetime import datetime
from pathlib import Path
from time import time

import git
import pandas as pd
from openpyxl import load_workbook
from openpyxl.cell.cell import Cell
from openpyxl.styles import Alignment, Font
from openpyxl.worksheet.worksheet import Worksheet
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn

parser = argparse.ArgumentParser(description="Benchmark LLVM scheduler configurations with GadgetSetAnalyzer.")
parser.add_argument("-b", "--benchmarks", default="all", help="Benchmarks to run (comma-separated list)")
parser.add_argument("-c", "--configs", default="all", help="Scheduling configs to run (comma-separated list: [pre-ra, post-ra, both])")
parser.add_argument("-f", "--flags", default="", help="Clang flags to use when compiling benchmarks")
parser.add_argument("-d", "--debug", action="store_true", help="Show command output")
parser.add_argument("-t", "--timeout", type=int, default=None, help="Timeout in seconds for a single command invocation")
parser.add_argument("--show-times", action="store_true", help="Display the time it takes for each benchmark to complete")
parser.add_argument("--skip-compilation", action="store_true", help="Skip the building and comparing steps, just combine the CSV files")
args = parser.parse_args()

configs = {
    "Default": "--misched=default --misched-postra=false",
    "Pre-RA": "--misched=ropsched --misched-postra=false",
    "Post-RA": "--misched=default --misched-postra=true",
    "Both": "--misched=ropsched --misched-postra=true",
}

cwd = Path(os.path.realpath(__file__)).expanduser().resolve().parent  # Everything is relative to where this script is actually located.
clang = cwd / "llvm-ropsched/build/bin/clang"
gsa = cwd / "GadgetSetAnalyzer"
results = cwd / "results"
binaries = results / "binaries"
output = results / "results.xlsx"
now = datetime.now().strftime("%b %d, %Y %I∶%M∶%S %p")

benchmarks = cwd / "benchmarks"
benchmarks = list(benchmarks.iterdir())

# Validate only available benchmarks were selected.
if args.benchmarks != "all":
    selected = args.benchmarks.split(",")
    available = [benchmark.name for benchmark in benchmarks]

    for benchmark in selected:
        if benchmark not in available:
            available = ",".join(available)
            parser.error(f"unknown benchmark '{benchmark}' not found in '{available}'")

    benchmarks = sorted(list(filter(lambda benchmark: benchmark.name in set(selected), benchmarks)))

# Validate only available configs were selected.
if args.configs != "all":
    selected = args.configs.split(",")
    available = set([name.lower() for name in configs.keys()])

    for config in selected:
        if config not in available:
            parser.error(f"unknown config '{config}'")

    for config in list(configs.keys()):
        if config.lower() != "default" and config.lower() not in selected:
            del configs[config]

def format(x: float | str) -> str:
    if isinstance(x, str):
        return x

    sign = "+" if x > 0 else ""
    return f"{sign}{x:.3f}"


def ftime(seconds: int):
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    seconds = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"


def highlight(ws: Worksheet, cell: Cell) -> None:
    header = ws.cell(1, cell.column).value
    value = str(cell.value)

    if ("+" in value and "Gadget Quality" in header) or ("-" in value and "Number" in header):
        cell.style = "Good"
    elif ("-" in value and "Gadget Quality" in header) or ("+" in value and "Number" in header):
        cell.style = "Bad"
    elif "(" in value:
        cell.style = "Neutral"


def run(cmd: str, cwd: Path, env: dict = None) -> None:
    stdout, stderr = (None, None) if args.debug else (subprocess.DEVNULL, subprocess.DEVNULL)
    subprocess.run(cmd, cwd=cwd, env=env, stdout=stdout, stderr=stderr, check=True, timeout=args.timeout)


def find(benchmark: str, release: Path) -> Path | None:
    filepath = release / benchmark

    # If there is an binary that has the same name as the benchmark, return that.
    if os.path.isfile(filepath) and os.access(filepath, os.X_OK):
        return filepath

    filenames = sorted(os.listdir(release))

    # Otherwise, prioritize any shared libary generated with the benchmark name.
    for filename in filenames:
        if benchmark in filename and filename.endswith(".so"):
            return release / filename


    # Otherwise, prioritize any binary artifact in the folder.
    for filename in filenames:
        filepath = release / filename
        if os.path.isfile(filepath) and os.access(filepath, os.X_OK):
            return filepath

    return None


def build(benchmark: Path, update: Callable[[str], None]) -> list[Path]:
    release = benchmark / "target" / "release"
    binary = None
    paths = []

    for config, misched in configs.items():
        if os.path.exists(release):
            shutil.rmtree(release)

        flags = misched.split(" ") + args.flags.split(" ")
        flags = [f"-C llvm-args={flag}" for flag in flags]
        flags = " ".join(flags)
        env = os.environ.copy()
        env["RUSTFLAGS"] = flags
        env["RUSTUP_TOOLCHAIN"] = "ropsched"

        update(config)  # Update the status bar with the current step.
        run(["cargo", "build", "--release"], benchmark, env=env)

        if binary is None:
            binary = find(benchmark.name, release)
            if binary is None:
                raise RuntimeError(f"could not find binary file for {benchmark.name}")

        filename = f"{benchmark.name}.{config}" if (benchmark.name == binary.name) else f"{benchmark.name} ({binary.name}).{config}"
        destination = binaries / filename
        shutil.copy2(release / binary, destination)
        paths.append(destination)

    return paths


def compare(original: Path, variants: list[Path]) -> Path:
    name = original.stem
    output = gsa / "results" / name
    variants = [f"{variant.suffix[1:]}={variant}" for variant in variants]

    if os.path.exists(output):
        shutil.rmtree(output)

    run(["python3", "src/GSA.py", str(original), "--variants", *variants, "--output_metrics", "--result_folder_name", name], cwd=gsa)

    return output / "Gadget Quality.csv"


def combine(files: list[Path]) -> Path:
    dataframes = []

    # Combine all the individual CSV files into a single dataframe.
    for csv in files:
        df = pd.read_csv(csv)
        df.insert(0, "Benchmark", csv.parent.name)
        dataframes.append(df)

    data = pd.concat(dataframes, ignore_index=True)

    # Convert the dataframe to an Excel workbook.
    if output.exists(): # Append to the existing results if the spreadsheet already exists.
        mode = "a"
        if_new_sheet_exists = "new"
    else:
        mode = "w"
        if_new_sheet_exists = None

    with pd.ExcelWriter(output, engine="openpyxl", mode=mode, if_sheet_exists=if_new_sheet_exists) as writer:
        data.to_excel(writer, sheet_name=now, index=False)

    # Format the results to be a little prettier.
    wb = load_workbook(output)
    ws = wb[wb.sheetnames[-1]]

    # Start by merging the benchmark names column.
    for row in range(2, ws.max_row, len(configs)):
        ws.merge_cells(start_row=row, start_column=1, end_row=row + len(configs) - 1, end_column=1)
        cell = ws.cell(row, 1)
        cell.alignment = Alignment(vertical="center", horizontal="center")
        cell.font = Font(bold=True)

    # Highlight the results based on whether it was positive, negative, or neutral.
    for row in ws.iter_cols(3):
        for cell in filter(lambda cell: cell.value is not None, row[1:]):
            highlight(ws, cell)


    def emphasize(cell: Cell) -> None:
        emphasis_cell = ws.cell(1, 1)

        for field in ["font", "border", "fill", "protection", "alignment"]:
            setattr(cell, field, copy(getattr(emphasis_cell, field)))

    row = ws.max_row + 2

    for col in range(1, ws.max_column + 1):
        header = ws.cell(1, col)
        cell = ws.cell(row, col)
        cell.value = header.value
        emphasize(cell)
    
    ws.cell(ws.max_row, 1).value = "Summary Metric"

    # Calculate the average results for each category accross each variant.
    number_of_scheduling_configs = len(configs)
    number_of_scheduling_variants = number_of_scheduling_configs - 1
    number_of_benchmarks = len(files)
    number_of_rop_metrics = 6

    def matrix(rows: int, columns: int) -> list[list[float]]:
        return [[0.0 for _ in range(columns)] for _ in range(rows)]

    def strip(value: str, difference: bool = False) -> float:
        if not difference:
            index = value.find("(") if "(" in value else len(value)
            stripped = value[:index]
        else:
            stripped = value[value.find("(") + 1:value.find(")")]

        return float(stripped)

    average_differences = matrix(number_of_scheduling_variants, number_of_rop_metrics)
    average_percent_differences = matrix(number_of_scheduling_variants, number_of_rop_metrics)
    average_weighted_percent_differences = matrix(number_of_scheduling_configs, number_of_rop_metrics)
    baseline = None

    for benchmark in range(number_of_benchmarks):
        start_row = benchmark * len(configs) + 2

        for c in range(number_of_rop_metrics):
            for r in range(number_of_scheduling_configs):
                row = ws[start_row + r][2:]
                cell = row[c]
                raw_value = str(cell.value)

                if c % 2 == 1:
                    gadget_quality = strip(raw_value)
                    gadget_count = strip(str(row[c - 1].value))
                    weighted_value = gadget_count * gadget_quality
                    average_weighted_percent_differences[r][c] += weighted_value
                else:
                    gaget_count = strip(raw_value)
                    average_weighted_percent_differences[r][c] += gaget_count

                if r == 0:
                    baseline = float(raw_value)
                else:
                    difference = strip(raw_value, True)
                    average_differences[r - 1][c] += difference
                    percent_difference = difference / baseline
                    average_percent_differences[r - 1][c] += percent_difference

    for r in range(number_of_scheduling_variants):
        for c in range(number_of_rop_metrics):
            average_differences[r][c] /= number_of_benchmarks
            average_percent_difference = average_percent_differences[r][c] / number_of_benchmarks
            average_percent_differences[r][c] = f"{'+' if average_percent_difference > 0 else ''}{average_percent_difference:.2%}"

    for r in range(1, number_of_scheduling_configs):
        for c in range(number_of_rop_metrics):
            percent_difference = 1 - average_weighted_percent_differences[r][c] / average_weighted_percent_differences[0][c]
            average_weighted_percent_differences[r][c] = f"{'+' if percent_difference > 0 else ''}{percent_difference:.2%}"

    def write_section(title: str, start_row: int, values: list[list]) -> None:
        ws.merge_cells(start_row=start_row, start_column=1, end_row=start_row + len(configs) - 2, end_column=1)
        cell = ws.cell(start_row, 1, title)
        cell.alignment = Alignment(vertical="center", horizontal="center")
        cell.font = Font(bold=True)

        for i, config in enumerate(list(configs.keys())[1:]):
            ws.cell(start_row + i, 2, config)

        for i in range(len(values)):
            for j in range(len(values[0])):
                cell = ws.cell(start_row + i, 3 + j, value=format(values[i][j]))
                highlight(ws, cell)

    write_section("Average Difference", ws.max_row + 1, average_differences)
    write_section("Average Percent Difference", ws.max_row + 1, average_percent_differences)
    write_section("Weighted Average Percent Difference", ws.max_row + 1, average_weighted_percent_differences[1:])

    # Write the metadata for the run.
    start_row = ws.max_row + 2

    ws.merge_cells(f"A{start_row}:I{start_row}")
    emphasize(ws[f"A{start_row}"])
    ws[f"A{start_row}"].value = "Run Information"

    start_row += 1
    this_git_repo_sha = git.Repo(cwd).head.commit.hexsha
    llvm_git_repo_sha = git.Repo(cwd / "llvm-ropsched").head.commit.hexsha
    rust_git_repo_sha = git.Repo(cwd / "rust-ropsched").head.commit.hexsha
    command = " ".join(sys.argv)

    metadata = {
        "Run Command": command,
        "Script Version": this_git_repo_sha,
        "LLVM Version": f"{llvm_git_repo_sha} (Forked from 21.1.5)",
        "Rust Version": f"{rust_git_repo_sha} (Forked from 1.91.1)",
        "OS": platform.system(),
        "Version": platform.version(),
        "Release": platform.release(),
        "Architecture": platform.machine(),
        "Python Version": sys.version,
    }

    for offset, (key, value) in enumerate(metadata.items()):
        row = start_row + offset
        ws.merge_cells(f"B{row}:I{row}")
        ws[f"A{row}"].alignment = Alignment(vertical="center", horizontal="center")
        ws[f"A{row}"].font = Font(bold=True)
        ws[f"A{row}"] = key
        ws[f"B{row}"] = value

    wb.save(output)

    return output


try:
    if not os.path.exists(binaries):
        os.makedirs(binaries)

    start = time()
    files = []

    with Progress(
        SpinnerColumn(style="bold cyan"),
        TextColumn("[bold white]Running Benchmarks[/]"),
        BarColumn(),
        TextColumn("[bold cyan]{task.completed}/{task.total}"),
        TextColumn("[bold magenta]{task.percentage:>3.0f}%"),
        TextColumn("[bold yellow]ETA: {task.fields[eta]}"),
        TextColumn("[bold cyan]Benchmark: [bold white]{task.fields[benchmark]} [bold cyan]-> {task.fields[step]}"),
        transient=True,
    ) as progress:

        task = progress.add_task("Running Benchmarks", total=len(benchmarks), eta="--:--", benchmark="---", step="---")

        if not args.skip_compilation:
            for benchmark in benchmarks:
                timer = time()
                progress.update(task, benchmark=benchmark.name)
                try:
                    paths = build(benchmark, lambda variant: progress.update(task, step=f"Compiling {variant} Variant"))
                    progress.update(task, step="Comparing Variants")
                    csv = compare(paths[0], paths[1:])
                    files.append(csv)
                except Exception as e:
                    print(f"Error with {benchmark.name}: {e}")

                completed = progress.tasks[task].completed + 1
                total = progress.tasks[task].total
                elapsed = time() - start

                if completed > 0:
                    rate = elapsed / completed
                    seconds = int(rate * (total - completed))
                    eta = ftime(seconds)
                else:
                    eta = "--:--:--"

                progress.update(task, advance=1, eta=eta)

                if args.show_times:
                    duration = int(time() - timer)
                    print(f"Finished {benchmark.name} in {ftime(duration)}")
        else:
            for benchmark in benchmarks:
                release = benchmark / "target" / "release"
                binary = find(benchmark.name, release)

                if binary is None:
                    raise RuntimeError(f"could not find binary file for {benchmark.name}")

                filename = f"{benchmark.name}" if (benchmark.name == binary.name) else f"{benchmark.name} ({binary.name})"
                csv = gsa / "results" / filename / "Gadget Quality.csv"
                files.append(csv)


    output = combine(files)
    print(f"Benchmark results compiled at {output}")

except KeyboardInterrupt:
    if files:
        print(f"Stopping early, compiling results from {len(files)}/{len(benchmarks)} benchmarks...")
        output = combine(files)
        print(f"Benchmark results compiled at {output}")

except Exception as exception:
    print(f"An unexpected error occurred: {exception}\n")

    import traceback
    traceback.print_exc()
